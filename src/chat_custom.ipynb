{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environments variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.streamlit/secrets.toml\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get database information using langchain.SQLDatabase\n",
    "Snowflake Database has 43 tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ORDERS_SAMPLE'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from snowflake.sqlalchemy import URL\n",
    "from langchain import OpenAI, SQLDatabase, SQLDatabaseChain\n",
    "\n",
    "# create snowflake connection uri\n",
    "uri_snow = URL(\n",
    "    account=os.getenv(\"account\"),\n",
    "    user=os.getenv(\"user\"),\n",
    "    password=os.getenv(\"password\"),\n",
    "    database=os.getenv(\"database\"),\n",
    "    schema=os.getenv(\"schema\"),\n",
    "    warehouse=os.getenv(\"warehouse\"),\n",
    "    role=os.getenv(\"role\"),\n",
    ")\n",
    "\n",
    "# generate prompt 2 tables\n",
    "# tables = [\"ecdc_global\", \"goog_global_mobility_report\", \"databank_demographics\", \"demographics\"]\n",
    "# db = SQLDatabase.from_uri(uri_snow, include_tables=tables)\n",
    "os.getenv(\"database\")\n",
    "# sample all tables\n",
    "# db = SQLDatabase.from_uri(uri_snow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCREATE TABLE customers (\\n\\tcustomer_id DECIMAL(38, 0) NOT NULL, \\n\\tcustomer_name VARCHAR(50), \\n\\taddress VARCHAR(100), \\n\\tphone_number VARCHAR(15), \\n\\tCONSTRAINT \"SYS_CONSTRAINT_5baa0130-6d36-4da3-bd78-919bb9520841\" PRIMARY KEY (customer_id)\\n)\\n\\n/*\\n3 rows from customers table:\\ncustomer_id\\tcustomer_name\\taddress\\tphone_number\\n1\\tJohn Doe\\t123 Main St\\t123-456-7890\\n2\\tJane Smith\\t456 Elm St\\t987-654-3210\\n3\\tMichael Johnson\\t789 Oak Ave\\t555-123-4567\\n*/\\n\\n\\nCREATE TABLE orderitems (\\n\\torder_item_id DECIMAL(38, 0) NOT NULL, \\n\\torder_id DECIMAL(38, 0), \\n\\tproduct_name VARCHAR(50), \\n\\tquantity DECIMAL(38, 0), \\n\\tCONSTRAINT \"SYS_CONSTRAINT_76efb4dc-883b-41fc-97f2-d91ebbbbefc2\" PRIMARY KEY (order_item_id), \\n\\tCONSTRAINT \"SYS_CONSTRAINT_f3b7502c-198a-4713-b687-9848113b5af3\" FOREIGN KEY(order_id) REFERENCES orders (order_id)\\n)\\n\\n/*\\n3 rows from orderitems table:\\norder_item_id\\torder_id\\tproduct_name\\tquantity\\n1\\t1\\tProduct A\\t2\\n2\\t2\\tProduct B\\t1\\n3\\t3\\tProduct C\\t3\\n*/\\n\\n\\nCREATE TABLE orders (\\n\\torder_id DECIMAL(38, 0) NOT NULL, \\n\\torder_date DATE, \\n\\tcustomer_id DECIMAL(38, 0), \\n\\tCONSTRAINT \"SYS_CONSTRAINT_b84b5812-b8d7-4b7d-969f-ffbf491cb22a\" PRIMARY KEY (order_id), \\n\\tCONSTRAINT \"SYS_CONSTRAINT_2774f2de-ce21-42ac-8b18-94935cbab902\" FOREIGN KEY(customer_id) REFERENCES customers (customer_id)\\n)\\n\\n/*\\n3 rows from orders table:\\norder_id\\torder_date\\tcustomer_id\\n1\\t2023-07-18\\t1\\n2\\t2023-07-19\\t2\\n3\\t2023-07-20\\t3\\n*/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get DDL and 3 rows samples for every table\n",
    "db_info = db.table_info\n",
    "db_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save database info into\n",
    "with open(\"../docs/database_info.txt\", \"w\") as fp:\n",
    "    fp.write(db_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "with open(\"../docs/database_info.txt\", \"r\") as fp:\n",
    "    text_file = fp.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create langchain documents from tables DDL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 438, which is longer than the specified 0\n",
      "Created a chunk of size 493, which is longer than the specified 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='CREATE TABLE customers (\\n\\tcustomer_id DECIMAL(38, 0) NOT NULL, \\n\\tcustomer_name VARCHAR(50), \\n\\taddress VARCHAR(100), \\n\\tphone_number VARCHAR(15), \\n\\tCONSTRAINT \"SYS_CONSTRAINT_5baa0130-6d36-4da3-bd78-919bb9520841\" PRIMARY KEY (customer_id)\\n)\\n\\n/*\\n3 rows from customers table:\\ncustomer_id\\tcustomer_name\\taddress\\tphone_number\\n1\\tJohn Doe\\t123 Main St\\t123-456-7890\\n2\\tJane Smith\\t456 Elm St\\t987-654-3210\\n3\\tMichael Johnson\\t789 Oak Ave\\t555-123-4567\\n*/', metadata={'document': 'database_info.txt'}),\n",
       " Document(page_content='CREATE TABLE orderitems (\\n\\torder_item_id DECIMAL(38, 0) NOT NULL, \\n\\torder_id DECIMAL(38, 0), \\n\\tproduct_name VARCHAR(50), \\n\\tquantity DECIMAL(38, 0), \\n\\tCONSTRAINT \"SYS_CONSTRAINT_76efb4dc-883b-41fc-97f2-d91ebbbbefc2\" PRIMARY KEY (order_item_id), \\n\\tCONSTRAINT \"SYS_CONSTRAINT_f3b7502c-198a-4713-b687-9848113b5af3\" FOREIGN KEY(order_id) REFERENCES orders (order_id)\\n)\\n\\n/*\\n3 rows from orderitems table:\\norder_item_id\\torder_id\\tproduct_name\\tquantity\\n1\\t1\\tProduct A\\t2\\n2\\t2\\tProduct B\\t1\\n3\\t3\\tProduct C\\t3\\n*/', metadata={'document': 'database_info.txt'}),\n",
       " Document(page_content='CREATE TABLE orders (\\n\\torder_id DECIMAL(38, 0) NOT NULL, \\n\\torder_date DATE, \\n\\tcustomer_id DECIMAL(38, 0), \\n\\tCONSTRAINT \"SYS_CONSTRAINT_b84b5812-b8d7-4b7d-969f-ffbf491cb22a\" PRIMARY KEY (order_id), \\n\\tCONSTRAINT \"SYS_CONSTRAINT_2774f2de-ce21-42ac-8b18-94935cbab902\" FOREIGN KEY(customer_id) REFERENCES customers (customer_id)\\n)\\n\\n/*\\n3 rows from orders table:\\norder_id\\torder_date\\tcustomer_id\\n1\\t2023-07-18\\t1\\n2\\t2023-07-19\\t2\\n3\\t2023-07-20\\t3\\n*/', metadata={'document': 'database_info.txt'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(        \n",
    "    separator = \"\\n\\n\\n\",\n",
    "    chunk_size = 0,\n",
    "    chunk_overlap = 0,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "# Split document\n",
    "# texts_split = text_file.split(\"\\n\\n\\n\")\n",
    "texts_split = text_splitter.split_text(text_file)\n",
    "metadatas = [ {\"document\": \"database_info.txt\"} for _ in texts_split]\n",
    "docs = text_splitter.create_documents(texts_split, metadatas)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embeddings and save database in local using Chroma and OpenAIEmbeddings\n",
    "- Generate the embeddings using openAI with documents from above cell \n",
    "- Create the database in Chroma (local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['1d215242-263a-11ee-91fe-ac7ed0d21e7b'],\n",
       " 'embeddings': None,\n",
       " 'documents': ['CREATE TABLE customers (\\n\\tcustomer_id DECIMAL(38, 0) NOT NULL, \\n\\tcustomer_name VARCHAR(50), \\n\\taddress VARCHAR(100), \\n\\tphone_number VARCHAR(15), \\n\\tCONSTRAINT \"SYS_CONSTRAINT_5baa0130-6d36-4da3-bd78-919bb9520841\" PRIMARY KEY (customer_id)\\n)\\n\\n/*\\n3 rows from customers table:\\ncustomer_id\\tcustomer_name\\taddress\\tphone_number\\n1\\tJohn Doe\\t123 Main St\\t123-456-7890\\n2\\tJane Smith\\t456 Elm St\\t987-654-3210\\n3\\tMichael Johnson\\t789 Oak Ave\\t555-123-4567\\n*/'],\n",
       " 'metadatas': [{'document': 'database_info.txt'}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import Chroma Library that allow to store vector database in local\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# remove folder and avoid conflicts\n",
    "! rmdir /s /q \"../chroma_db\"\n",
    "\n",
    "# create object for embeddings using OpenAI\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# create database and save embeddings in local\n",
    "vector_store = Chroma.from_documents(docs, embeddings, persist_directory=\"../chroma_db\")\n",
    "vector_store.persist()\n",
    "\n",
    "# get first embedding in database \n",
    "# vector_store.get(limit=1, include=['embeddings', 'documents', 'metadatas'])\n",
    "vector_store.get(limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store._collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='CREATE TABLE ecdc_global (\\n\\tcountry_region VARCHAR(16777216), \\n\\tcontinentexp VARCHAR(16777216), \\n\\tiso3166_1 VARCHAR(2), \\n\\tcases FLOAT, \\n\\tdeaths FLOAT, \\n\\tcases_since_prev_day FLOAT, \\n\\tdeaths_since_prev_day FLOAT, \\n\\tpopulation FLOAT, \\n\\tdate DATE, \\n\\tlast_update_date TIMESTAMP_NTZ, \\n\\tlast_reported_flag BOOLEAN\\n)\\n\\n/*\\n3 rows from ecdc_global table:\\ncountry_region\\tcontinentexp\\tiso3166_1\\tcases\\tdeaths\\tcases_since_prev_day\\tdeaths_since_prev_day\\tpopulation\\tdate\\tlast_update_date\\tlast_reported_flag\\nAfghanistan\\tAsia\\tAF\\t746.0\\t6.0\\t0.0\\t0.0\\t38041757.0\\t2020-12-14\\t2023-07-18 00:03:41.837110\\tTrue\\nAfghanistan\\tAsia\\tAF\\t298.0\\t9.0\\t-448.0\\t3.0\\t38041757.0\\t2020-12-13\\t2023-07-18 00:03:41.837110\\tFalse\\nAfghanistan\\tAsia\\tAF\\t113.0\\t11.0\\t-185.0\\t2.0\\t38041757.0\\t2020-12-12\\t2023-07-18 00:03:41.837110\\tFalse\\n*/', metadata={'document': 'database_info.txt'}),\n",
       " Document(page_content='CREATE TABLE goog_global_mobility_report (\\n\\tcountry_region VARCHAR(250), \\n\\tprovince_state VARCHAR(250), \\n\\tiso_3166_1 VARCHAR(2), \\n\\tiso_3166_2 VARCHAR(5), \\n\\tdate DATE, \\n\\tgrocery_and_pharmacy_change_perc FLOAT, \\n\\tparks_change_perc FLOAT, \\n\\tresidential_change_perc FLOAT, \\n\\tretail_and_recreation_change_perc FLOAT, \\n\\ttransit_stations_change_perc FLOAT, \\n\\tworkplaces_change_perc FLOAT, \\n\\tlast_update_date TIMESTAMP_NTZ, \\n\\tlast_reported_flag BOOLEAN, \\n\\tsub_region_2 VARCHAR(256)\\n)\\n\\n/*\\n3 rows from goog_global_mobility_report table:\\ncountry_region\\tprovince_state\\tiso_3166_1\\tiso_3166_2\\tdate\\tgrocery_and_pharmacy_change_perc\\tparks_change_perc\\tresidential_change_perc\\tretail_and_recreation_change_perc\\ttransit_stations_change_perc\\tworkplaces_change_perc\\tlast_update_date\\tlast_reported_flag\\tsub_region_2\\nUnited States\\tTennessee\\tUS\\tTN\\t2022-02-24\\t6.0\\t-29.0\\t6.0\\t-10.0\\t-30.0\\t-7.0\\t2023-07-18 00:04:35.415163\\tFalse\\tMontgomery County\\nUnited States\\tTennessee\\tUS\\tTN\\t2022-02-25\\t3.0\\t-17.0\\t4.0\\t-6.0\\t-27.0\\t1.0\\t2023-07-18 00:04:35.415163\\tFalse\\tMontgomery County\\nUnited States\\tTennessee\\tUS\\tTN\\t2022-02-26\\t10.0\\t58.0\\t1.0\\t0.0\\t-23.0\\t11.0\\t2023-07-18 00:04:35.415163\\tFalse\\tMontgomery County\\n*/', metadata={'document': 'database_info.txt'})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load from disk \n",
    "vector_store = Chroma(persist_directory=\"../chroma_db\", embedding_function=embeddings)\n",
    "docs = vector_store.similarity_search(\"which table contains the names of countries\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='CREATE TABLE ecdc_global (\\n\\tcountry_region VARCHAR(16777216), \\n\\tcontinentexp VARCHAR(16777216), \\n\\tiso3166_1 VARCHAR(2), \\n\\tcases FLOAT, \\n\\tdeaths FLOAT, \\n\\tcases_since_prev_day FLOAT, \\n\\tdeaths_since_prev_day FLOAT, \\n\\tpopulation FLOAT, \\n\\tdate DATE, \\n\\tlast_update_date TIMESTAMP_NTZ, \\n\\tlast_reported_flag BOOLEAN\\n)\\n\\n/*\\n3 rows from ecdc_global table:\\ncountry_region\\tcontinentexp\\tiso3166_1\\tcases\\tdeaths\\tcases_since_prev_day\\tdeaths_since_prev_day\\tpopulation\\tdate\\tlast_update_date\\tlast_reported_flag\\nAfghanistan\\tAsia\\tAF\\t746.0\\t6.0\\t0.0\\t0.0\\t38041757.0\\t2020-12-14\\t2023-07-18 00:03:41.837110\\tTrue\\nAfghanistan\\tAsia\\tAF\\t298.0\\t9.0\\t-448.0\\t3.0\\t38041757.0\\t2020-12-13\\t2023-07-18 00:03:41.837110\\tFalse\\nAfghanistan\\tAsia\\tAF\\t113.0\\t11.0\\t-185.0\\t2.0\\t38041757.0\\t2020-12-12\\t2023-07-18 00:03:41.837110\\tFalse\\n*/', metadata={'document': 'database_info.txt'}),\n",
       " Document(page_content='CREATE TABLE goog_global_mobility_report (\\n\\tcountry_region VARCHAR(250), \\n\\tprovince_state VARCHAR(250), \\n\\tiso_3166_1 VARCHAR(2), \\n\\tiso_3166_2 VARCHAR(5), \\n\\tdate DATE, \\n\\tgrocery_and_pharmacy_change_perc FLOAT, \\n\\tparks_change_perc FLOAT, \\n\\tresidential_change_perc FLOAT, \\n\\tretail_and_recreation_change_perc FLOAT, \\n\\ttransit_stations_change_perc FLOAT, \\n\\tworkplaces_change_perc FLOAT, \\n\\tlast_update_date TIMESTAMP_NTZ, \\n\\tlast_reported_flag BOOLEAN, \\n\\tsub_region_2 VARCHAR(256)\\n)\\n\\n/*\\n3 rows from goog_global_mobility_report table:\\ncountry_region\\tprovince_state\\tiso_3166_1\\tiso_3166_2\\tdate\\tgrocery_and_pharmacy_change_perc\\tparks_change_perc\\tresidential_change_perc\\tretail_and_recreation_change_perc\\ttransit_stations_change_perc\\tworkplaces_change_perc\\tlast_update_date\\tlast_reported_flag\\tsub_region_2\\nUnited States\\tTennessee\\tUS\\tTN\\t2022-02-24\\t6.0\\t-29.0\\t6.0\\t-10.0\\t-30.0\\t-7.0\\t2023-07-18 00:04:35.415163\\tFalse\\tMontgomery County\\nUnited States\\tTennessee\\tUS\\tTN\\t2022-02-25\\t3.0\\t-17.0\\t4.0\\t-6.0\\t-27.0\\t1.0\\t2023-07-18 00:04:35.415163\\tFalse\\tMontgomery County\\nUnited States\\tTennessee\\tUS\\tTN\\t2022-02-26\\t10.0\\t58.0\\t1.0\\t0.0\\t-23.0\\t11.0\\t2023-07-18 00:04:35.415163\\tFalse\\tMontgomery County\\n*/', metadata={'document': 'database_info.txt'})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search document which better match from question\n",
    "vector_store.similarity_search(\"which table country has more deaths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "template_questions = \"\"\"Considering the provided chat history and a subsequent question, rewrite the follow-up question to be an independent query. Alternatively, conclude the conversation if it appears to be complete.\n",
    "Chat History:\\\"\"\"\n",
    "{chat_history}\n",
    "\\\"\"\"\n",
    "Follow Up Input: \\\"\"\"\n",
    "{question}\n",
    "\\\"\"\"\n",
    "Standalone question:\"\"\"\n",
    "\n",
    "\n",
    "template_qa = \"\"\" \n",
    "You're an AI assistant specializing in data analysis with Snowflake SQL. When providing responses, strive to exhibit friendliness and adopt a conversational tone, similar to how a friend or tutor would communicate.\n",
    "When asked about your capabilities, provide a general overview of your ability to assist with data analysis tasks using Snowflake SQL, instead of performing specific SQL queries. \n",
    "Based on the question provided, if it pertains to data analysis or SQL tasks, generate SQL code that is compatible with the Snowflake environment. Additionally, offer a brief explanation about how you arrived at the SQL code. If the required column isn't explicitly stated in the context, suggest an alternative using available columns, but do not assume the existence of any columns that are not mentioned. Also, do not modify the database in any way (no insert, update, or delete operations). You are only allowed to query the database. Refrain from using the information schema.\n",
    "If the question or context does not clearly involve SQL or data analysis tasks, respond appropriately without generating SQL queries. \n",
    "When the user expresses gratitude or says \"Thanks\", interpret it as a signal to conclude the conversation. Respond with an appropriate closing statement without generating further SQL queries.\n",
    "If you don't know the answer, simply state, \"I'm sorry, I don't know the answer to your question.\"\n",
    "Write your response in markdown format.\n",
    "\n",
    "Question: ```{question}```\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "condense_question_prompt = PromptTemplate.from_template(template_questions)\n",
    "prompt_qa = PromptTemplate(template=template_qa, input_variables=[\"question\", \"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['chat_history', 'question'], output_parser=None, partial_variables={}, template='Considering the provided chat history and a subsequent question, rewrite the follow-up question to be an independent query. Alternatively, conclude the conversation if it appears to be complete.\\nChat History:\"\"\"\\n{chat_history}\\n\"\"\"\\nFollow Up Input: \"\"\"\\n{question}\\n\"\"\"\\nStandalone question:', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condense_question_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "q_llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-16k\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=500,\n",
    "    # streaming=True,\n",
    ")\n",
    "\n",
    "question_generator = LLMChain(llm=q_llm, prompt=condense_question_prompt)\n",
    "doc_chain = load_qa_chain(llm=llm, chain_type=\"stuff\", prompt=prompt_qa)\n",
    "\n",
    "conv_chain = ConversationalRetrievalChain(\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    combine_docs_chain=doc_chain,\n",
    "    question_generator=question_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! I'm an AI assistant specializing in data analysis with Snowflake SQL. I can help you with various data analysis tasks using SQL queries in the Snowflake environment. Here's a brief overview of the two databases mentioned:\n",
      "\n",
      "1. `ecdc_global` table:\n",
      "   - This table contains information about COVID-19 cases and deaths globally.\n",
      "   - It includes columns such as `country_region`, `continentexp`, `iso3166_1`, `cases`, `deaths`, `cases_since_prev_day`, `deaths_since_prev_day`, `population`, `date`, `last_update_date`, and `last_reported_flag`.\n",
      "\n",
      "2. `goog_global_mobility_report` table:\n",
      "   - This table provides mobility data for different regions.\n",
      "   - It includes columns such as `country_region`, `province_state`, `iso_3166_1`, `iso_3166_2`, `date`, `grocery_and_pharmacy_change_perc`, `parks_change_perc`, `residential_change_perc`, `retail_and_recreation_change_perc`, `transit_stations_change_perc`, `workplaces_change_perc`, `last_update_date`, `last_reported_flag`, and `sub_region_2`.\n",
      "\n",
      "Now, here are three example questions you can ask about these tables:\n",
      "\n",
      "1. What is the total number of COVID-19 cases and deaths for each country in the `ecdc_global` table?\n",
      "2. How has the mobility in parks changed over time in the United States according to the `goog_global_mobility_report` table?\n",
      "3. Which country has the highest percentage change in grocery and pharmacy visits on a specific date in the `goog_global_mobility_report` table?\n",
      "\n",
      "Feel free to ask me any specific question related to data analysis or SQL tasks using Snowflake SQL!\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "question = \"\"\"Now to get started, please briefly introduce yourself, describe the database at a high level. Then provide 3 example questions using bullet points. this reponse without query. Write your response in markdown format.\"\"\"\n",
    "result = conv_chain(\n",
    "            {\"question\": question, \"chat_history\": chat_history}\n",
    "        )\n",
    "answer = result[\"answer\"]\n",
    "\n",
    "# store the response in chat history\n",
    "chat_history = [(question, answer)]\n",
    "\n",
    "# show answer\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To obtain the total number of COVID-19 cases and deaths for each country in the `ecdc_global` table, you can use the following SQL query:\n",
      "\n",
      "```sql\n",
      "SELECT country_region, SUM(cases) AS total_cases, SUM(deaths) AS total_deaths\n",
      "FROM ecdc_global\n",
      "GROUP BY country_region;\n",
      "```\n",
      "\n",
      "This query selects the `country_region` column and calculates the sum of the `cases` and `deaths` columns for each unique country using the `SUM()` function. The result is grouped by the `country_region` column using the `GROUP BY` clause.\n",
      "\n",
      "Please note that this query assumes that the `ecdc_global` table contains the necessary data and columns mentioned in the question. If there are any additional requirements or if you need further assistance, please let me know.\n"
     ]
    }
   ],
   "source": [
    "result = conv_chain(\n",
    "            {\"question\": \"What is the total number of COVID-19 cases and deaths for each country in the `ecdc_global` table?\", \"chat_history\": []}\n",
    "        )\n",
    "answer = result[\"answer\"]\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
